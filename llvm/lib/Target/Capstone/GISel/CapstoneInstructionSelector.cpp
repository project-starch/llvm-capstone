//===-- CapstoneInstructionSelector.cpp -----------------------------*- C++ -*-==//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
/// \file
/// This file implements the targeting of the InstructionSelector class for
/// Capstone.
/// \todo This should be generated by TableGen.
//===----------------------------------------------------------------------===//

#include "MCTargetDesc/CapstoneMatInt.h"
#include "CapstoneRegisterBankInfo.h"
#include "CapstoneSubtarget.h"
#include "CapstoneTargetMachine.h"
#include "llvm/CodeGen/GlobalISel/GIMatchTableExecutorImpl.h"
#include "llvm/CodeGen/GlobalISel/GISelValueTracking.h"
#include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
#include "llvm/CodeGen/GlobalISel/InstructionSelector.h"
#include "llvm/CodeGen/GlobalISel/MIPatternMatch.h"
#include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
#include "llvm/CodeGen/MachineJumpTableInfo.h"
#include "llvm/IR/IntrinsicsCapstone.h"
#include "llvm/Support/Debug.h"

#define DEBUG_TYPE "capstone-isel"

using namespace llvm;
using namespace MIPatternMatch;

#define GET_GLOBALISEL_PREDICATE_BITSET
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATE_BITSET

namespace {

class CapstoneInstructionSelector : public InstructionSelector {
public:
  CapstoneInstructionSelector(const CapstoneTargetMachine &TM,
                           const CapstoneSubtarget &STI,
                           const CapstoneRegisterBankInfo &RBI);

  bool select(MachineInstr &MI) override;

  void setupMF(MachineFunction &MF, GISelValueTracking *VT,
               CodeGenCoverage *CoverageInfo, ProfileSummaryInfo *PSI,
               BlockFrequencyInfo *BFI) override {
    InstructionSelector::setupMF(MF, VT, CoverageInfo, PSI, BFI);
    MRI = &MF.getRegInfo();
  }

  static const char *getName() { return DEBUG_TYPE; }

private:
  const TargetRegisterClass *
  getRegClassForTypeOnBank(LLT Ty, const RegisterBank &RB) const;

  static constexpr unsigned MaxRecursionDepth = 6;

  bool hasAllNBitUsers(const MachineInstr &MI, unsigned Bits,
                       const unsigned Depth = 0) const;
  bool hasAllHUsers(const MachineInstr &MI) const {
    return hasAllNBitUsers(MI, 16);
  }
  bool hasAllWUsers(const MachineInstr &MI) const {
    return hasAllNBitUsers(MI, 32);
  }

  bool isRegInGprb(Register Reg) const;
  bool isRegInFprb(Register Reg) const;

  // tblgen-erated 'select' implementation, used as the initial selector for
  // the patterns that don't require complex C++.
  bool selectImpl(MachineInstr &I, CodeGenCoverage &CoverageInfo) const;

  // A lowering phase that runs before any selection attempts.
  // Returns true if the instruction was modified.
  void preISelLower(MachineInstr &MI, MachineIRBuilder &MIB);

  bool replacePtrWithInt(MachineOperand &Op, MachineIRBuilder &MIB);

  // Custom selection methods
  bool selectCopy(MachineInstr &MI) const;
  bool selectImplicitDef(MachineInstr &MI, MachineIRBuilder &MIB) const;
  bool materializeImm(Register Reg, int64_t Imm, MachineIRBuilder &MIB) const;
  bool selectAddr(MachineInstr &MI, MachineIRBuilder &MIB, bool IsLocal = true,
                  bool IsExternWeak = false) const;
  bool selectSelect(MachineInstr &MI, MachineIRBuilder &MIB) const;
  bool selectFPCompare(MachineInstr &MI, MachineIRBuilder &MIB) const;
  void emitFence(AtomicOrdering FenceOrdering, SyncScope::ID FenceSSID,
                 MachineIRBuilder &MIB) const;
  bool selectUnmergeValues(MachineInstr &MI, MachineIRBuilder &MIB) const;

  ComplexRendererFns selectShiftMask(MachineOperand &Root,
                                     unsigned ShiftWidth) const;
  ComplexRendererFns selectShiftMaskXLen(MachineOperand &Root) const {
    return selectShiftMask(Root, STI.getXLen());
  }
  ComplexRendererFns selectShiftMask32(MachineOperand &Root) const {
    return selectShiftMask(Root, 32);
  }
  ComplexRendererFns selectAddrRegImm(MachineOperand &Root) const;

  ComplexRendererFns selectSExtBits(MachineOperand &Root, unsigned Bits) const;
  template <unsigned Bits>
  ComplexRendererFns selectSExtBits(MachineOperand &Root) const {
    return selectSExtBits(Root, Bits);
  }

  ComplexRendererFns selectZExtBits(MachineOperand &Root, unsigned Bits) const;
  template <unsigned Bits>
  ComplexRendererFns selectZExtBits(MachineOperand &Root) const {
    return selectZExtBits(Root, Bits);
  }

  ComplexRendererFns selectSHXADDOp(MachineOperand &Root, unsigned ShAmt) const;
  template <unsigned ShAmt>
  ComplexRendererFns selectSHXADDOp(MachineOperand &Root) const {
    return selectSHXADDOp(Root, ShAmt);
  }

  ComplexRendererFns selectSHXADD_UWOp(MachineOperand &Root,
                                       unsigned ShAmt) const;
  template <unsigned ShAmt>
  ComplexRendererFns selectSHXADD_UWOp(MachineOperand &Root) const {
    return selectSHXADD_UWOp(Root, ShAmt);
  }

  ComplexRendererFns renderVLOp(MachineOperand &Root) const;

  // Custom renderers for tablegen
  void renderNegImm(MachineInstrBuilder &MIB, const MachineInstr &MI,
                    int OpIdx) const;
  void renderImmSubFromXLen(MachineInstrBuilder &MIB, const MachineInstr &MI,
                            int OpIdx) const;
  void renderImmSubFrom32(MachineInstrBuilder &MIB, const MachineInstr &MI,
                          int OpIdx) const;
  void renderImmPlus1(MachineInstrBuilder &MIB, const MachineInstr &MI,
                      int OpIdx) const;
  void renderFrameIndex(MachineInstrBuilder &MIB, const MachineInstr &MI,
                        int OpIdx) const;

  void renderTrailingZeros(MachineInstrBuilder &MIB, const MachineInstr &MI,
                           int OpIdx) const;
  void renderXLenSubTrailingOnes(MachineInstrBuilder &MIB,
                                 const MachineInstr &MI, int OpIdx) const;

  void renderAddiPairImmLarge(MachineInstrBuilder &MIB, const MachineInstr &MI,
                              int OpIdx) const;
  void renderAddiPairImmSmall(MachineInstrBuilder &MIB, const MachineInstr &MI,
                              int OpIdx) const;

  const CapstoneSubtarget &STI;
  const CapstoneInstrInfo &TII;
  const CapstoneRegisterInfo &TRI;
  const CapstoneRegisterBankInfo &RBI;
  const CapstoneTargetMachine &TM;

  MachineRegisterInfo *MRI = nullptr;

  // FIXME: This is necessary because DAGISel uses "Subtarget->" and GlobalISel
  // uses "STI." in the code generated by TableGen. We need to unify the name of
  // Subtarget variable.
  const CapstoneSubtarget *Subtarget = &STI;

#define GET_GLOBALISEL_PREDICATES_DECL
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATES_DECL

#define GET_GLOBALISEL_TEMPORARIES_DECL
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_TEMPORARIES_DECL
};

} // end anonymous namespace

#define GET_GLOBALISEL_IMPL
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_IMPL

CapstoneInstructionSelector::CapstoneInstructionSelector(
    const CapstoneTargetMachine &TM, const CapstoneSubtarget &STI,
    const CapstoneRegisterBankInfo &RBI)
    : STI(STI), TII(*STI.getInstrInfo()), TRI(*STI.getRegisterInfo()), RBI(RBI),
      TM(TM),

#define GET_GLOBALISEL_PREDICATES_INIT
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_PREDICATES_INIT
#define GET_GLOBALISEL_TEMPORARIES_INIT
#include "CapstoneGenGlobalISel.inc"
#undef GET_GLOBALISEL_TEMPORARIES_INIT
{
}

// Mimics optimizations in ISel and CapstoneOptWInst Pass
bool CapstoneInstructionSelector::hasAllNBitUsers(const MachineInstr &MI,
                                               unsigned Bits,
                                               const unsigned Depth) const {

  assert((MI.getOpcode() == TargetOpcode::G_ADD ||
          MI.getOpcode() == TargetOpcode::G_SUB ||
          MI.getOpcode() == TargetOpcode::G_MUL ||
          MI.getOpcode() == TargetOpcode::G_SHL ||
          MI.getOpcode() == TargetOpcode::G_LSHR ||
          MI.getOpcode() == TargetOpcode::G_AND ||
          MI.getOpcode() == TargetOpcode::G_OR ||
          MI.getOpcode() == TargetOpcode::G_XOR ||
          MI.getOpcode() == TargetOpcode::G_SEXT_INREG || Depth != 0) &&
         "Unexpected opcode");

  if (Depth >= CapstoneInstructionSelector::MaxRecursionDepth)
    return false;

  auto DestReg = MI.getOperand(0).getReg();
  for (auto &UserOp : MRI->use_nodbg_operands(DestReg)) {
    assert(UserOp.getParent() && "UserOp must have a parent");
    const MachineInstr &UserMI = *UserOp.getParent();
    unsigned OpIdx = UserOp.getOperandNo();

    switch (UserMI.getOpcode()) {
    default:
      return false;
    case Capstone::ADDW:
    case Capstone::ADDIW:
    case Capstone::SUBW:
      if (Bits >= 32)
        break;
      return false;
    case Capstone::SLL:
    case Capstone::SRA:
    case Capstone::SRL:
      // Shift amount operands only use log2(Xlen) bits.
      if (OpIdx == 2 && Bits >= Log2_32(Subtarget->getXLen()))
        break;
      return false;
    case Capstone::SLLI:
      // SLLI only uses the lower (XLen - ShAmt) bits.
      if (Bits >= Subtarget->getXLen() - UserMI.getOperand(2).getImm())
        break;
      return false;
    case Capstone::ANDI:
      if (Bits >= (unsigned)llvm::bit_width<uint64_t>(
                      (uint64_t)UserMI.getOperand(2).getImm()))
        break;
      goto RecCheck;
    case Capstone::AND:
    case Capstone::OR:
    case Capstone::XOR:
    RecCheck:
      if (hasAllNBitUsers(UserMI, Bits, Depth + 1))
        break;
      return false;
    case Capstone::SRLI: {
      unsigned ShAmt = UserMI.getOperand(2).getImm();
      // If we are shifting right by less than Bits, and users don't demand any
      // bits that were shifted into [Bits-1:0], then we can consider this as an
      // N-Bit user.
      if (Bits > ShAmt && hasAllNBitUsers(UserMI, Bits - ShAmt, Depth + 1))
        break;
      return false;
    }
    }
  }

  return true;
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectShiftMask(MachineOperand &Root,
                                          unsigned ShiftWidth) const {
  if (!Root.isReg())
    return std::nullopt;

  using namespace llvm::MIPatternMatch;

  Register ShAmtReg = Root.getReg();
  // Peek through zext.
  Register ZExtSrcReg;
  if (mi_match(ShAmtReg, *MRI, m_GZExt(m_Reg(ZExtSrcReg))))
    ShAmtReg = ZExtSrcReg;

  APInt AndMask;
  Register AndSrcReg;
  // Try to combine the following pattern (applicable to other shift
  // instructions as well as 32-bit ones):
  //
  //   %4:gprb(s64) = G_AND %3, %2
  //   %5:gprb(s64) = G_LSHR %1, %4(s64)
  //
  // According to Capstone's ISA manual, SLL, SRL, and SRA ignore other bits than
  // the lowest log2(XLEN) bits of register rs2. As for the above pattern, if
  // the lowest log2(XLEN) bits of register rd and rs2 of G_AND are the same,
  // then it can be eliminated. Given register rs1 or rs2 holding a constant
  // (the and mask), there are two cases G_AND can be erased:
  //
  // 1. the lowest log2(XLEN) bits of the and mask are all set
  // 2. the bits of the register being masked are already unset (zero set)
  if (mi_match(ShAmtReg, *MRI, m_GAnd(m_Reg(AndSrcReg), m_ICst(AndMask)))) {
    APInt ShMask(AndMask.getBitWidth(), ShiftWidth - 1);
    if (ShMask.isSubsetOf(AndMask)) {
      ShAmtReg = AndSrcReg;
    } else {
      // SimplifyDemandedBits may have optimized the mask so try restoring any
      // bits that are known zero.
      KnownBits Known = VT->getKnownBits(AndSrcReg);
      if (ShMask.isSubsetOf(AndMask | Known.Zero))
        ShAmtReg = AndSrcReg;
    }
  }

  APInt Imm;
  Register Reg;
  if (mi_match(ShAmtReg, *MRI, m_GAdd(m_Reg(Reg), m_ICst(Imm)))) {
    if (Imm != 0 && Imm.urem(ShiftWidth) == 0)
      // If we are shifting by X+N where N == 0 mod Size, then just shift by X
      // to avoid the ADD.
      ShAmtReg = Reg;
  } else if (mi_match(ShAmtReg, *MRI, m_GSub(m_ICst(Imm), m_Reg(Reg)))) {
    if (Imm != 0 && Imm.urem(ShiftWidth) == 0) {
      // If we are shifting by N-X where N == 0 mod Size, then just shift by -X
      // to generate a NEG instead of a SUB of a constant.
      ShAmtReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
      unsigned NegOpc = Subtarget->is64Bit() ? Capstone::SUBW : Capstone::SUB;
      return {{[=](MachineInstrBuilder &MIB) {
        MachineIRBuilder(*MIB.getInstr())
            .buildInstr(NegOpc, {ShAmtReg}, {Register(Capstone::X0), Reg});
        MIB.addReg(ShAmtReg);
      }}};
    }
    if (Imm.urem(ShiftWidth) == ShiftWidth - 1) {
      // If we are shifting by N-X where N == -1 mod Size, then just shift by ~X
      // to generate a NOT instead of a SUB of a constant.
      ShAmtReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
      return {{[=](MachineInstrBuilder &MIB) {
        MachineIRBuilder(*MIB.getInstr())
            .buildInstr(Capstone::XORI, {ShAmtReg}, {Reg})
            .addImm(-1);
        MIB.addReg(ShAmtReg);
      }}};
    }
  }

  return {{[=](MachineInstrBuilder &MIB) { MIB.addReg(ShAmtReg); }}};
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectSExtBits(MachineOperand &Root,
                                         unsigned Bits) const {
  if (!Root.isReg())
    return std::nullopt;
  Register RootReg = Root.getReg();
  MachineInstr *RootDef = MRI->getVRegDef(RootReg);

  if (RootDef->getOpcode() == TargetOpcode::G_SEXT_INREG &&
      RootDef->getOperand(2).getImm() == Bits) {
    return {
        {[=](MachineInstrBuilder &MIB) { MIB.add(RootDef->getOperand(1)); }}};
  }

  unsigned Size = MRI->getType(RootReg).getScalarSizeInBits();
  if ((Size - VT->computeNumSignBits(RootReg)) < Bits)
    return {{[=](MachineInstrBuilder &MIB) { MIB.add(Root); }}};

  return std::nullopt;
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectZExtBits(MachineOperand &Root,
                                         unsigned Bits) const {
  if (!Root.isReg())
    return std::nullopt;
  Register RootReg = Root.getReg();

  Register RegX;
  uint64_t Mask = maskTrailingOnes<uint64_t>(Bits);
  if (mi_match(RootReg, *MRI, m_GAnd(m_Reg(RegX), m_SpecificICst(Mask)))) {
    return {{[=](MachineInstrBuilder &MIB) { MIB.addReg(RegX); }}};
  }

  if (mi_match(RootReg, *MRI, m_GZExt(m_Reg(RegX))) &&
      MRI->getType(RegX).getScalarSizeInBits() == Bits)
    return {{[=](MachineInstrBuilder &MIB) { MIB.addReg(RegX); }}};

  unsigned Size = MRI->getType(RootReg).getScalarSizeInBits();
  if (VT->maskedValueIsZero(RootReg, APInt::getBitsSetFrom(Size, Bits)))
    return {{[=](MachineInstrBuilder &MIB) { MIB.add(Root); }}};

  return std::nullopt;
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectSHXADDOp(MachineOperand &Root,
                                         unsigned ShAmt) const {
  using namespace llvm::MIPatternMatch;

  if (!Root.isReg())
    return std::nullopt;
  Register RootReg = Root.getReg();

  const unsigned XLen = STI.getXLen();
  APInt Mask, C2;
  Register RegY;
  std::optional<bool> LeftShift;
  // (and (shl y, c2), mask)
  if (mi_match(RootReg, *MRI,
               m_GAnd(m_GShl(m_Reg(RegY), m_ICst(C2)), m_ICst(Mask))))
    LeftShift = true;
  // (and (lshr y, c2), mask)
  else if (mi_match(RootReg, *MRI,
                    m_GAnd(m_GLShr(m_Reg(RegY), m_ICst(C2)), m_ICst(Mask))))
    LeftShift = false;

  if (LeftShift.has_value()) {
    if (*LeftShift)
      Mask &= maskTrailingZeros<uint64_t>(C2.getLimitedValue());
    else
      Mask &= maskTrailingOnes<uint64_t>(XLen - C2.getLimitedValue());

    if (Mask.isShiftedMask()) {
      unsigned Leading = XLen - Mask.getActiveBits();
      unsigned Trailing = Mask.countr_zero();
      // Given (and (shl y, c2), mask) in which mask has no leading zeros and
      // c3 trailing zeros. We can use an SRLI by c3 - c2 followed by a SHXADD.
      if (*LeftShift && Leading == 0 && C2.ult(Trailing) && Trailing == ShAmt) {
        Register DstReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
        return {{[=](MachineInstrBuilder &MIB) {
          MachineIRBuilder(*MIB.getInstr())
              .buildInstr(Capstone::SRLI, {DstReg}, {RegY})
              .addImm(Trailing - C2.getLimitedValue());
          MIB.addReg(DstReg);
        }}};
      }

      // Given (and (lshr y, c2), mask) in which mask has c2 leading zeros and
      // c3 trailing zeros. We can use an SRLI by c2 + c3 followed by a SHXADD.
      if (!*LeftShift && Leading == C2 && Trailing == ShAmt) {
        Register DstReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
        return {{[=](MachineInstrBuilder &MIB) {
          MachineIRBuilder(*MIB.getInstr())
              .buildInstr(Capstone::SRLI, {DstReg}, {RegY})
              .addImm(Leading + Trailing);
          MIB.addReg(DstReg);
        }}};
      }
    }
  }

  LeftShift.reset();

  // (shl (and y, mask), c2)
  if (mi_match(RootReg, *MRI,
               m_GShl(m_OneNonDBGUse(m_GAnd(m_Reg(RegY), m_ICst(Mask))),
                      m_ICst(C2))))
    LeftShift = true;
  // (lshr (and y, mask), c2)
  else if (mi_match(RootReg, *MRI,
                    m_GLShr(m_OneNonDBGUse(m_GAnd(m_Reg(RegY), m_ICst(Mask))),
                            m_ICst(C2))))
    LeftShift = false;

  if (LeftShift.has_value() && Mask.isShiftedMask()) {
    unsigned Leading = XLen - Mask.getActiveBits();
    unsigned Trailing = Mask.countr_zero();

    // Given (shl (and y, mask), c2) in which mask has 32 leading zeros and
    // c3 trailing zeros. If c1 + c3 == ShAmt, we can emit SRLIW + SHXADD.
    bool Cond = *LeftShift && Leading == 32 && Trailing > 0 &&
                (Trailing + C2.getLimitedValue()) == ShAmt;
    if (!Cond)
      // Given (lshr (and y, mask), c2) in which mask has 32 leading zeros and
      // c3 trailing zeros. If c3 - c1 == ShAmt, we can emit SRLIW + SHXADD.
      Cond = !*LeftShift && Leading == 32 && C2.ult(Trailing) &&
             (Trailing - C2.getLimitedValue()) == ShAmt;

    if (Cond) {
      Register DstReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
      return {{[=](MachineInstrBuilder &MIB) {
        MachineIRBuilder(*MIB.getInstr())
            .buildInstr(Capstone::SRLIW, {DstReg}, {RegY})
            .addImm(Trailing);
        MIB.addReg(DstReg);
      }}};
    }
  }

  return std::nullopt;
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectSHXADD_UWOp(MachineOperand &Root,
                                            unsigned ShAmt) const {
  using namespace llvm::MIPatternMatch;

  if (!Root.isReg())
    return std::nullopt;
  Register RootReg = Root.getReg();

  // Given (and (shl x, c2), mask) in which mask is a shifted mask with
  // 32 - ShAmt leading zeros and c2 trailing zeros. We can use SLLI by
  // c2 - ShAmt followed by SHXADD_UW with ShAmt for x amount.
  APInt Mask, C2;
  Register RegX;
  if (mi_match(
          RootReg, *MRI,
          m_OneNonDBGUse(m_GAnd(m_OneNonDBGUse(m_GShl(m_Reg(RegX), m_ICst(C2))),
                                m_ICst(Mask))))) {
    Mask &= maskTrailingZeros<uint64_t>(C2.getLimitedValue());

    if (Mask.isShiftedMask()) {
      unsigned Leading = Mask.countl_zero();
      unsigned Trailing = Mask.countr_zero();
      if (Leading == 32 - ShAmt && C2 == Trailing && Trailing > ShAmt) {
        Register DstReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
        return {{[=](MachineInstrBuilder &MIB) {
          MachineIRBuilder(*MIB.getInstr())
              .buildInstr(Capstone::SLLI, {DstReg}, {RegX})
              .addImm(C2.getLimitedValue() - ShAmt);
          MIB.addReg(DstReg);
        }}};
      }
    }
  }

  return std::nullopt;
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::renderVLOp(MachineOperand &Root) const {
  assert(Root.isReg() && "Expected operand to be a Register");
  MachineInstr *RootDef = MRI->getVRegDef(Root.getReg());

  if (RootDef->getOpcode() == TargetOpcode::G_CONSTANT) {
    auto C = RootDef->getOperand(1).getCImm();
    if (C->getValue().isAllOnes())
      // If the operand is a G_CONSTANT with value of all ones it is larger than
      // VLMAX. We convert it to an immediate with value VLMaxSentinel. This is
      // recognized specially by the vsetvli insertion pass.
      return {{[=](MachineInstrBuilder &MIB) {
        MIB.addImm(Capstone::VLMaxSentinel);
      }}};

    if (isUInt<5>(C->getZExtValue())) {
      uint64_t ZExtC = C->getZExtValue();
      return {{[=](MachineInstrBuilder &MIB) { MIB.addImm(ZExtC); }}};
    }
  }
  return {{[=](MachineInstrBuilder &MIB) { MIB.addReg(Root.getReg()); }}};
}

InstructionSelector::ComplexRendererFns
CapstoneInstructionSelector::selectAddrRegImm(MachineOperand &Root) const {
  if (!Root.isReg())
    return std::nullopt;

  MachineInstr *RootDef = MRI->getVRegDef(Root.getReg());
  if (RootDef->getOpcode() == TargetOpcode::G_FRAME_INDEX) {
    return {{
        [=](MachineInstrBuilder &MIB) { MIB.add(RootDef->getOperand(1)); },
        [=](MachineInstrBuilder &MIB) { MIB.addImm(0); },
    }};
  }

  if (isBaseWithConstantOffset(Root, *MRI)) {
    MachineOperand &LHS = RootDef->getOperand(1);
    MachineOperand &RHS = RootDef->getOperand(2);
    MachineInstr *LHSDef = MRI->getVRegDef(LHS.getReg());
    MachineInstr *RHSDef = MRI->getVRegDef(RHS.getReg());

    int64_t RHSC = RHSDef->getOperand(1).getCImm()->getSExtValue();
    if (isInt<12>(RHSC)) {
      if (LHSDef->getOpcode() == TargetOpcode::G_FRAME_INDEX)
        return {{
            [=](MachineInstrBuilder &MIB) { MIB.add(LHSDef->getOperand(1)); },
            [=](MachineInstrBuilder &MIB) { MIB.addImm(RHSC); },
        }};

      return {{[=](MachineInstrBuilder &MIB) { MIB.add(LHS); },
               [=](MachineInstrBuilder &MIB) { MIB.addImm(RHSC); }}};
    }
  }

  // TODO: Need to get the immediate from a G_PTR_ADD. Should this be done in
  // the combiner?
  return {{[=](MachineInstrBuilder &MIB) { MIB.addReg(Root.getReg()); },
           [=](MachineInstrBuilder &MIB) { MIB.addImm(0); }}};
}

/// Returns the CapstoneCC::CondCode that corresponds to the CmpInst::Predicate CC.
/// CC Must be an ICMP Predicate.
static CapstoneCC::CondCode getCapstoneCCFromICmp(CmpInst::Predicate CC) {
  switch (CC) {
  default:
    llvm_unreachable("Expected ICMP CmpInst::Predicate.");
  case CmpInst::Predicate::ICMP_EQ:
    return CapstoneCC::COND_EQ;
  case CmpInst::Predicate::ICMP_NE:
    return CapstoneCC::COND_NE;
  case CmpInst::Predicate::ICMP_ULT:
    return CapstoneCC::COND_LTU;
  case CmpInst::Predicate::ICMP_SLT:
    return CapstoneCC::COND_LT;
  case CmpInst::Predicate::ICMP_UGE:
    return CapstoneCC::COND_GEU;
  case CmpInst::Predicate::ICMP_SGE:
    return CapstoneCC::COND_GE;
  }
}

static void getOperandsForBranch(Register CondReg, CapstoneCC::CondCode &CC,
                                 Register &LHS, Register &RHS,
                                 MachineRegisterInfo &MRI) {
  // Try to fold an ICmp. If that fails, use a NE compare with X0.
  CmpInst::Predicate Pred = CmpInst::BAD_ICMP_PREDICATE;
  if (!mi_match(CondReg, MRI, m_GICmp(m_Pred(Pred), m_Reg(LHS), m_Reg(RHS)))) {
    LHS = CondReg;
    RHS = Capstone::X0;
    CC = CapstoneCC::COND_NE;
    return;
  }

  // We found an ICmp, do some canonicalization.

  // Adjust comparisons to use comparison with 0 if possible.
  if (auto Constant = getIConstantVRegSExtVal(RHS, MRI)) {
    switch (Pred) {
    case CmpInst::Predicate::ICMP_SGT:
      // Convert X > -1 to X >= 0
      if (*Constant == -1) {
        CC = CapstoneCC::COND_GE;
        RHS = Capstone::X0;
        return;
      }
      break;
    case CmpInst::Predicate::ICMP_SLT:
      // Convert X < 1 to 0 >= X
      if (*Constant == 1) {
        CC = CapstoneCC::COND_GE;
        RHS = LHS;
        LHS = Capstone::X0;
        return;
      }
      break;
    default:
      break;
    }
  }

  switch (Pred) {
  default:
    llvm_unreachable("Expected ICMP CmpInst::Predicate.");
  case CmpInst::Predicate::ICMP_EQ:
  case CmpInst::Predicate::ICMP_NE:
  case CmpInst::Predicate::ICMP_ULT:
  case CmpInst::Predicate::ICMP_SLT:
  case CmpInst::Predicate::ICMP_UGE:
  case CmpInst::Predicate::ICMP_SGE:
    // These CCs are supported directly by Capstone branches.
    break;
  case CmpInst::Predicate::ICMP_SGT:
  case CmpInst::Predicate::ICMP_SLE:
  case CmpInst::Predicate::ICMP_UGT:
  case CmpInst::Predicate::ICMP_ULE:
    // These CCs are not supported directly by Capstone branches, but changing the
    // direction of the CC and swapping LHS and RHS are.
    Pred = CmpInst::getSwappedPredicate(Pred);
    std::swap(LHS, RHS);
    break;
  }

  CC = getCapstoneCCFromICmp(Pred);
}

bool CapstoneInstructionSelector::select(MachineInstr &MI) {
  MachineIRBuilder MIB(MI);

  preISelLower(MI, MIB);
  const unsigned Opc = MI.getOpcode();

  if (!MI.isPreISelOpcode() || Opc == TargetOpcode::G_PHI) {
    if (Opc == TargetOpcode::PHI || Opc == TargetOpcode::G_PHI) {
      const Register DefReg = MI.getOperand(0).getReg();
      const LLT DefTy = MRI->getType(DefReg);

      const RegClassOrRegBank &RegClassOrBank =
          MRI->getRegClassOrRegBank(DefReg);

      const TargetRegisterClass *DefRC =
          dyn_cast<const TargetRegisterClass *>(RegClassOrBank);
      if (!DefRC) {
        if (!DefTy.isValid()) {
          LLVM_DEBUG(dbgs() << "PHI operand has no type, not a gvreg?\n");
          return false;
        }

        const RegisterBank &RB = *cast<const RegisterBank *>(RegClassOrBank);
        DefRC = getRegClassForTypeOnBank(DefTy, RB);
        if (!DefRC) {
          LLVM_DEBUG(dbgs() << "PHI operand has unexpected size/bank\n");
          return false;
        }
      }

      MI.setDesc(TII.get(TargetOpcode::PHI));
      return RBI.constrainGenericRegister(DefReg, *DefRC, *MRI);
    }

    // Certain non-generic instructions also need some special handling.
    if (MI.isCopy())
      return selectCopy(MI);

    return true;
  }

  if (selectImpl(MI, *CoverageInfo))
    return true;

  switch (Opc) {
  case TargetOpcode::G_ANYEXT:
  case TargetOpcode::G_PTRTOINT:
  case TargetOpcode::G_INTTOPTR:
  case TargetOpcode::G_TRUNC:
  case TargetOpcode::G_FREEZE:
    return selectCopy(MI);
  case TargetOpcode::G_CONSTANT: {
    Register DstReg = MI.getOperand(0).getReg();
    int64_t Imm = MI.getOperand(1).getCImm()->getSExtValue();

    if (!materializeImm(DstReg, Imm, MIB))
      return false;

    MI.eraseFromParent();
    return true;
  }
  case TargetOpcode::G_FCONSTANT: {
    // TODO: Use constant pool for complex constants.
    Register DstReg = MI.getOperand(0).getReg();
    const APFloat &FPimm = MI.getOperand(1).getFPImm()->getValueAPF();
    unsigned Size = MRI->getType(DstReg).getSizeInBits();
    if (Size == 16 || Size == 32 || (Size == 64 && Subtarget->is64Bit())) {
      Register GPRReg;
      if (FPimm.isPosZero()) {
        GPRReg = Capstone::X0;
      } else {
        GPRReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
        APInt Imm = FPimm.bitcastToAPInt();
        if (!materializeImm(GPRReg, Imm.getSExtValue(), MIB))
          return false;
      }

      unsigned Opcode = Size == 64   ? Capstone::FMV_D_X
                        : Size == 32 ? Capstone::FMV_W_X
                                     : Capstone::FMV_H_X;
      auto FMV = MIB.buildInstr(Opcode, {DstReg}, {GPRReg});
      if (!FMV.constrainAllUses(TII, TRI, RBI))
        return false;
    } else {
      // s64 on rv32
      assert(Size == 64 && !Subtarget->is64Bit() &&
             "Unexpected size or subtarget");

      if (FPimm.isPosZero()) {
        // Optimize +0.0 to use fcvt.d.w
        MachineInstrBuilder FCVT =
            MIB.buildInstr(Capstone::FCVT_D_W, {DstReg}, {Register(Capstone::X0)})
                .addImm(CapstoneFPRndMode::RNE);
        if (!FCVT.constrainAllUses(TII, TRI, RBI))
          return false;

        MI.eraseFromParent();
        return true;
      }

      // Split into two pieces and build through the stack.
      Register GPRRegHigh = MRI->createVirtualRegister(&Capstone::GPRRegClass);
      Register GPRRegLow = MRI->createVirtualRegister(&Capstone::GPRRegClass);
      APInt Imm = FPimm.bitcastToAPInt();
      if (!materializeImm(GPRRegHigh, Imm.extractBits(32, 32).getSExtValue(),
                          MIB))
        return false;
      if (!materializeImm(GPRRegLow, Imm.trunc(32).getSExtValue(), MIB))
        return false;
      MachineInstrBuilder PairF64 = MIB.buildInstr(
          Capstone::BuildPairF64Pseudo, {DstReg}, {GPRRegLow, GPRRegHigh});
      if (!PairF64.constrainAllUses(TII, TRI, RBI))
        return false;
    }

    MI.eraseFromParent();
    return true;
  }
  case TargetOpcode::G_GLOBAL_VALUE: {
    auto *GV = MI.getOperand(1).getGlobal();
    if (GV->isThreadLocal()) {
      // TODO: implement this case.
      return false;
    }

    return selectAddr(MI, MIB, GV->isDSOLocal(), GV->hasExternalWeakLinkage());
  }
  case TargetOpcode::G_JUMP_TABLE:
  case TargetOpcode::G_CONSTANT_POOL:
    return selectAddr(MI, MIB, MRI);
  case TargetOpcode::G_BRCOND: {
    Register LHS, RHS;
    CapstoneCC::CondCode CC;
    getOperandsForBranch(MI.getOperand(0).getReg(), CC, LHS, RHS, *MRI);

    auto Bcc = MIB.buildInstr(CapstoneCC::getBrCond(CC), {}, {LHS, RHS})
                   .addMBB(MI.getOperand(1).getMBB());
    MI.eraseFromParent();
    return constrainSelectedInstRegOperands(*Bcc, TII, TRI, RBI);
  }
  case TargetOpcode::G_BRINDIRECT:
    MI.setDesc(TII.get(Capstone::PseudoBRIND));
    MI.addOperand(MachineOperand::CreateImm(0));
    return constrainSelectedInstRegOperands(MI, TII, TRI, RBI);
  case TargetOpcode::G_SELECT:
    return selectSelect(MI, MIB);
  case TargetOpcode::G_FCMP:
    return selectFPCompare(MI, MIB);
  case TargetOpcode::G_FENCE: {
    AtomicOrdering FenceOrdering =
        static_cast<AtomicOrdering>(MI.getOperand(0).getImm());
    SyncScope::ID FenceSSID =
        static_cast<SyncScope::ID>(MI.getOperand(1).getImm());
    emitFence(FenceOrdering, FenceSSID, MIB);
    MI.eraseFromParent();
    return true;
  }
  case TargetOpcode::G_IMPLICIT_DEF:
    return selectImplicitDef(MI, MIB);
  case TargetOpcode::G_UNMERGE_VALUES:
    return selectUnmergeValues(MI, MIB);
  default:
    return false;
  }
}

bool CapstoneInstructionSelector::selectUnmergeValues(
    MachineInstr &MI, MachineIRBuilder &MIB) const {
  assert(MI.getOpcode() == TargetOpcode::G_UNMERGE_VALUES);

  if (!Subtarget->hasStdExtZfa())
    return false;

  // Split F64 Src into two s32 parts
  if (MI.getNumOperands() != 3)
    return false;
  Register Src = MI.getOperand(2).getReg();
  Register Lo = MI.getOperand(0).getReg();
  Register Hi = MI.getOperand(1).getReg();
  if (!isRegInFprb(Src) || !isRegInGprb(Lo) || !isRegInGprb(Hi))
    return false;

  MachineInstr *ExtractLo = MIB.buildInstr(Capstone::FMV_X_W_FPR64, {Lo}, {Src});
  if (!constrainSelectedInstRegOperands(*ExtractLo, TII, TRI, RBI))
    return false;

  MachineInstr *ExtractHi = MIB.buildInstr(Capstone::FMVH_X_D, {Hi}, {Src});
  if (!constrainSelectedInstRegOperands(*ExtractHi, TII, TRI, RBI))
    return false;

  MI.eraseFromParent();
  return true;
}

bool CapstoneInstructionSelector::replacePtrWithInt(MachineOperand &Op,
                                                 MachineIRBuilder &MIB) {
  Register PtrReg = Op.getReg();
  assert(MRI->getType(PtrReg).isPointer() && "Operand is not a pointer!");

  const LLT sXLen = LLT::scalar(STI.getXLen());
  auto PtrToInt = MIB.buildPtrToInt(sXLen, PtrReg);
  MRI->setRegBank(PtrToInt.getReg(0), RBI.getRegBank(Capstone::GPRBRegBankID));
  Op.setReg(PtrToInt.getReg(0));
  return select(*PtrToInt);
}

void CapstoneInstructionSelector::preISelLower(MachineInstr &MI,
                                            MachineIRBuilder &MIB) {
  switch (MI.getOpcode()) {
  case TargetOpcode::G_PTR_ADD: {
    Register DstReg = MI.getOperand(0).getReg();
    const LLT sXLen = LLT::scalar(STI.getXLen());

    replacePtrWithInt(MI.getOperand(1), MIB);
    MI.setDesc(TII.get(TargetOpcode::G_ADD));
    MRI->setType(DstReg, sXLen);
    break;
  }
  case TargetOpcode::G_PTRMASK: {
    Register DstReg = MI.getOperand(0).getReg();
    const LLT sXLen = LLT::scalar(STI.getXLen());
    replacePtrWithInt(MI.getOperand(1), MIB);
    MI.setDesc(TII.get(TargetOpcode::G_AND));
    MRI->setType(DstReg, sXLen);
    break;
  }
  }
}

void CapstoneInstructionSelector::renderNegImm(MachineInstrBuilder &MIB,
                                            const MachineInstr &MI,
                                            int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  int64_t CstVal = MI.getOperand(1).getCImm()->getSExtValue();
  MIB.addImm(-CstVal);
}

void CapstoneInstructionSelector::renderImmSubFromXLen(MachineInstrBuilder &MIB,
                                                    const MachineInstr &MI,
                                                    int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  uint64_t CstVal = MI.getOperand(1).getCImm()->getZExtValue();
  MIB.addImm(STI.getXLen() - CstVal);
}

void CapstoneInstructionSelector::renderImmSubFrom32(MachineInstrBuilder &MIB,
                                                  const MachineInstr &MI,
                                                  int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  uint64_t CstVal = MI.getOperand(1).getCImm()->getZExtValue();
  MIB.addImm(32 - CstVal);
}

void CapstoneInstructionSelector::renderImmPlus1(MachineInstrBuilder &MIB,
                                              const MachineInstr &MI,
                                              int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  int64_t CstVal = MI.getOperand(1).getCImm()->getSExtValue();
  MIB.addImm(CstVal + 1);
}

void CapstoneInstructionSelector::renderFrameIndex(MachineInstrBuilder &MIB,
                                                const MachineInstr &MI,
                                                int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_FRAME_INDEX && OpIdx == -1 &&
         "Expected G_FRAME_INDEX");
  MIB.add(MI.getOperand(1));
}

void CapstoneInstructionSelector::renderTrailingZeros(MachineInstrBuilder &MIB,
                                                   const MachineInstr &MI,
                                                   int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  uint64_t C = MI.getOperand(1).getCImm()->getZExtValue();
  MIB.addImm(llvm::countr_zero(C));
}

void CapstoneInstructionSelector::renderXLenSubTrailingOnes(
    MachineInstrBuilder &MIB, const MachineInstr &MI, int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  uint64_t C = MI.getOperand(1).getCImm()->getZExtValue();
  MIB.addImm(Subtarget->getXLen() - llvm::countr_one(C));
}

void CapstoneInstructionSelector::renderAddiPairImmSmall(MachineInstrBuilder &MIB,
                                                      const MachineInstr &MI,
                                                      int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  int64_t Imm = MI.getOperand(1).getCImm()->getSExtValue();
  int64_t Adj = Imm < 0 ? -2048 : 2047;
  MIB.addImm(Imm - Adj);
}

void CapstoneInstructionSelector::renderAddiPairImmLarge(MachineInstrBuilder &MIB,
                                                      const MachineInstr &MI,
                                                      int OpIdx) const {
  assert(MI.getOpcode() == TargetOpcode::G_CONSTANT && OpIdx == -1 &&
         "Expected G_CONSTANT");
  int64_t Imm = MI.getOperand(1).getCImm()->getSExtValue() < 0 ? -2048 : 2047;
  MIB.addImm(Imm);
}

const TargetRegisterClass *CapstoneInstructionSelector::getRegClassForTypeOnBank(
    LLT Ty, const RegisterBank &RB) const {
  if (RB.getID() == Capstone::GPRBRegBankID) {
    if (Ty.getSizeInBits() <= 32 || (STI.is64Bit() && Ty.getSizeInBits() == 64))
      return &Capstone::GPRRegClass;
  }

  if (RB.getID() == Capstone::FPRBRegBankID) {
    if (Ty.getSizeInBits() == 16)
      return &Capstone::FPR16RegClass;
    if (Ty.getSizeInBits() == 32)
      return &Capstone::FPR32RegClass;
    if (Ty.getSizeInBits() == 64)
      return &Capstone::FPR64RegClass;
  }

  if (RB.getID() == Capstone::VRBRegBankID) {
    if (Ty.getSizeInBits().getKnownMinValue() <= 64)
      return &Capstone::VRRegClass;

    if (Ty.getSizeInBits().getKnownMinValue() == 128)
      return &Capstone::VRM2RegClass;

    if (Ty.getSizeInBits().getKnownMinValue() == 256)
      return &Capstone::VRM4RegClass;

    if (Ty.getSizeInBits().getKnownMinValue() == 512)
      return &Capstone::VRM8RegClass;
  }

  return nullptr;
}

bool CapstoneInstructionSelector::isRegInGprb(Register Reg) const {
  return RBI.getRegBank(Reg, *MRI, TRI)->getID() == Capstone::GPRBRegBankID;
}

bool CapstoneInstructionSelector::isRegInFprb(Register Reg) const {
  return RBI.getRegBank(Reg, *MRI, TRI)->getID() == Capstone::FPRBRegBankID;
}

bool CapstoneInstructionSelector::selectCopy(MachineInstr &MI) const {
  Register DstReg = MI.getOperand(0).getReg();

  if (DstReg.isPhysical())
    return true;

  const TargetRegisterClass *DstRC = getRegClassForTypeOnBank(
      MRI->getType(DstReg), *RBI.getRegBank(DstReg, *MRI, TRI));
  assert(DstRC &&
         "Register class not available for LLT, register bank combination");

  // No need to constrain SrcReg. It will get constrained when
  // we hit another of its uses or its defs.
  // Copies do not have constraints.
  if (!RBI.constrainGenericRegister(DstReg, *DstRC, *MRI)) {
    LLVM_DEBUG(dbgs() << "Failed to constrain " << TII.getName(MI.getOpcode())
                      << " operand\n");
    return false;
  }

  MI.setDesc(TII.get(Capstone::COPY));
  return true;
}

bool CapstoneInstructionSelector::selectImplicitDef(MachineInstr &MI,
                                                 MachineIRBuilder &MIB) const {
  assert(MI.getOpcode() == TargetOpcode::G_IMPLICIT_DEF);

  const Register DstReg = MI.getOperand(0).getReg();
  const TargetRegisterClass *DstRC = getRegClassForTypeOnBank(
      MRI->getType(DstReg), *RBI.getRegBank(DstReg, *MRI, TRI));

  assert(DstRC &&
         "Register class not available for LLT, register bank combination");

  if (!RBI.constrainGenericRegister(DstReg, *DstRC, *MRI)) {
    LLVM_DEBUG(dbgs() << "Failed to constrain " << TII.getName(MI.getOpcode())
                      << " operand\n");
  }
  MI.setDesc(TII.get(TargetOpcode::IMPLICIT_DEF));
  return true;
}

bool CapstoneInstructionSelector::materializeImm(Register DstReg, int64_t Imm,
                                              MachineIRBuilder &MIB) const {
  if (Imm == 0) {
    MIB.buildCopy(DstReg, Register(Capstone::X0));
    RBI.constrainGenericRegister(DstReg, Capstone::GPRRegClass, *MRI);
    return true;
  }

  CapstoneMatInt::InstSeq Seq = CapstoneMatInt::generateInstSeq(Imm, *Subtarget);
  unsigned NumInsts = Seq.size();
  Register SrcReg = Capstone::X0;

  for (unsigned i = 0; i < NumInsts; i++) {
    Register TmpReg = i < NumInsts - 1
                          ? MRI->createVirtualRegister(&Capstone::GPRRegClass)
                          : DstReg;
    const CapstoneMatInt::Inst &I = Seq[i];
    MachineInstr *Result;

    switch (I.getOpndKind()) {
    case CapstoneMatInt::Imm:
      // clang-format off
      Result = MIB.buildInstr(I.getOpcode(), {TmpReg}, {})
                   .addImm(I.getImm());
      // clang-format on
      break;
    case CapstoneMatInt::RegX0:
      Result = MIB.buildInstr(I.getOpcode(), {TmpReg},
                              {SrcReg, Register(Capstone::X0)});
      break;
    case CapstoneMatInt::RegReg:
      Result = MIB.buildInstr(I.getOpcode(), {TmpReg}, {SrcReg, SrcReg});
      break;
    case CapstoneMatInt::RegImm:
      Result =
          MIB.buildInstr(I.getOpcode(), {TmpReg}, {SrcReg}).addImm(I.getImm());
      break;
    }

    if (!constrainSelectedInstRegOperands(*Result, TII, TRI, RBI))
      return false;

    SrcReg = TmpReg;
  }

  return true;
}

bool CapstoneInstructionSelector::selectAddr(MachineInstr &MI,
                                          MachineIRBuilder &MIB, bool IsLocal,
                                          bool IsExternWeak) const {
  assert((MI.getOpcode() == TargetOpcode::G_GLOBAL_VALUE ||
          MI.getOpcode() == TargetOpcode::G_JUMP_TABLE ||
          MI.getOpcode() == TargetOpcode::G_CONSTANT_POOL) &&
         "Unexpected opcode");

  const MachineOperand &DispMO = MI.getOperand(1);

  Register DefReg = MI.getOperand(0).getReg();
  const LLT DefTy = MRI->getType(DefReg);

  // When HWASAN is used and tagging of global variables is enabled
  // they should be accessed via the GOT, since the tagged address of a global
  // is incompatible with existing code models. This also applies to non-pic
  // mode.
  if (TM.isPositionIndependent() || Subtarget->allowTaggedGlobals()) {
    if (IsLocal && !Subtarget->allowTaggedGlobals()) {
      // Use PC-relative addressing to access the symbol. This generates the
      // pattern (PseudoLLA sym), which expands to (addi (auipc %pcrel_hi(sym))
      // %pcrel_lo(auipc)).
      MI.setDesc(TII.get(Capstone::PseudoLLA));
      return constrainSelectedInstRegOperands(MI, TII, TRI, RBI);
    }

    // Use PC-relative addressing to access the GOT for this symbol, then
    // load the address from the GOT. This generates the pattern (PseudoLGA
    // sym), which expands to (ld (addi (auipc %got_pcrel_hi(sym))
    // %pcrel_lo(auipc))).
    MachineFunction &MF = *MI.getParent()->getParent();
    MachineMemOperand *MemOp = MF.getMachineMemOperand(
        MachinePointerInfo::getGOT(MF),
        MachineMemOperand::MOLoad | MachineMemOperand::MODereferenceable |
            MachineMemOperand::MOInvariant,
        DefTy, Align(DefTy.getSizeInBits() / 8));

    auto Result = MIB.buildInstr(Capstone::PseudoLGA, {DefReg}, {})
                      .addDisp(DispMO, 0)
                      .addMemOperand(MemOp);

    if (!constrainSelectedInstRegOperands(*Result, TII, TRI, RBI))
      return false;

    MI.eraseFromParent();
    return true;
  }

  switch (TM.getCodeModel()) {
  default: {
    reportGISelFailure(*MF, *TPC, *MORE, getName(),
                       "Unsupported code model for lowering", MI);
    return false;
  }
  case CodeModel::Small: {
    // Must lie within a single 2 GiB address range and must lie between
    // absolute addresses -2 GiB and +2 GiB. This generates the pattern (addi
    // (lui %hi(sym)) %lo(sym)).
    Register AddrHiDest = MRI->createVirtualRegister(&Capstone::GPRRegClass);
    MachineInstr *AddrHi = MIB.buildInstr(Capstone::LUI, {AddrHiDest}, {})
                               .addDisp(DispMO, 0, CapstoneII::MO_HI);

    if (!constrainSelectedInstRegOperands(*AddrHi, TII, TRI, RBI))
      return false;

    auto Result = MIB.buildInstr(Capstone::ADDI, {DefReg}, {AddrHiDest})
                      .addDisp(DispMO, 0, CapstoneII::MO_LO);

    if (!constrainSelectedInstRegOperands(*Result, TII, TRI, RBI))
      return false;

    MI.eraseFromParent();
    return true;
  }
  case CodeModel::Medium:
    // Emit LGA/LLA instead of the sequence it expands to because the pcrel_lo
    // relocation needs to reference a label that points to the auipc
    // instruction itself, not the global. This cannot be done inside the
    // instruction selector.
    if (IsExternWeak) {
      // An extern weak symbol may be undefined, i.e. have value 0, which may
      // not be within 2GiB of PC, so use GOT-indirect addressing to access the
      // symbol. This generates the pattern (PseudoLGA sym), which expands to
      // (ld (addi (auipc %got_pcrel_hi(sym)) %pcrel_lo(auipc))).
      MachineFunction &MF = *MI.getParent()->getParent();
      MachineMemOperand *MemOp = MF.getMachineMemOperand(
          MachinePointerInfo::getGOT(MF),
          MachineMemOperand::MOLoad | MachineMemOperand::MODereferenceable |
              MachineMemOperand::MOInvariant,
          DefTy, Align(DefTy.getSizeInBits() / 8));

      auto Result = MIB.buildInstr(Capstone::PseudoLGA, {DefReg}, {})
                        .addDisp(DispMO, 0)
                        .addMemOperand(MemOp);

      if (!constrainSelectedInstRegOperands(*Result, TII, TRI, RBI))
        return false;

      MI.eraseFromParent();
      return true;
    }

    // Generate a sequence for accessing addresses within any 2GiB range
    // within the address space. This generates the pattern (PseudoLLA sym),
    // which expands to (addi (auipc %pcrel_hi(sym)) %pcrel_lo(auipc)).
    MI.setDesc(TII.get(Capstone::PseudoLLA));
    return constrainSelectedInstRegOperands(MI, TII, TRI, RBI);
  }

  return false;
}

bool CapstoneInstructionSelector::selectSelect(MachineInstr &MI,
                                            MachineIRBuilder &MIB) const {
  auto &SelectMI = cast<GSelect>(MI);

  Register LHS, RHS;
  CapstoneCC::CondCode CC;
  getOperandsForBranch(SelectMI.getCondReg(), CC, LHS, RHS, *MRI);

  Register DstReg = SelectMI.getReg(0);

  unsigned Opc = Capstone::Select_GPR_Using_CC_GPR;
  if (RBI.getRegBank(DstReg, *MRI, TRI)->getID() == Capstone::FPRBRegBankID) {
    unsigned Size = MRI->getType(DstReg).getSizeInBits();
    Opc = Size == 32 ? Capstone::Select_FPR32_Using_CC_GPR
                     : Capstone::Select_FPR64_Using_CC_GPR;
  }

  MachineInstr *Result = MIB.buildInstr(Opc)
                             .addDef(DstReg)
                             .addReg(LHS)
                             .addReg(RHS)
                             .addImm(CC)
                             .addReg(SelectMI.getTrueReg())
                             .addReg(SelectMI.getFalseReg());
  MI.eraseFromParent();
  return constrainSelectedInstRegOperands(*Result, TII, TRI, RBI);
}

// Convert an FCMP predicate to one of the supported F or D instructions.
static unsigned getFCmpOpcode(CmpInst::Predicate Pred, unsigned Size) {
  assert((Size == 16 || Size == 32 || Size == 64) && "Unsupported size");
  switch (Pred) {
  default:
    llvm_unreachable("Unsupported predicate");
  case CmpInst::FCMP_OLT:
    return Size == 16 ? Capstone::FLT_H : Size == 32 ? Capstone::FLT_S : Capstone::FLT_D;
  case CmpInst::FCMP_OLE:
    return Size == 16 ? Capstone::FLE_H : Size == 32 ? Capstone::FLE_S : Capstone::FLE_D;
  case CmpInst::FCMP_OEQ:
    return Size == 16 ? Capstone::FEQ_H : Size == 32 ? Capstone::FEQ_S : Capstone::FEQ_D;
  }
}

// Try legalizing an FCMP by swapping or inverting the predicate to one that
// is supported.
static bool legalizeFCmpPredicate(Register &LHS, Register &RHS,
                                  CmpInst::Predicate &Pred, bool &NeedInvert) {
  auto isLegalFCmpPredicate = [](CmpInst::Predicate Pred) {
    return Pred == CmpInst::FCMP_OLT || Pred == CmpInst::FCMP_OLE ||
           Pred == CmpInst::FCMP_OEQ;
  };

  assert(!isLegalFCmpPredicate(Pred) && "Predicate already legal?");

  CmpInst::Predicate InvPred = CmpInst::getSwappedPredicate(Pred);
  if (isLegalFCmpPredicate(InvPred)) {
    Pred = InvPred;
    std::swap(LHS, RHS);
    return true;
  }

  InvPred = CmpInst::getInversePredicate(Pred);
  NeedInvert = true;
  if (isLegalFCmpPredicate(InvPred)) {
    Pred = InvPred;
    return true;
  }
  InvPred = CmpInst::getSwappedPredicate(InvPred);
  if (isLegalFCmpPredicate(InvPred)) {
    Pred = InvPred;
    std::swap(LHS, RHS);
    return true;
  }

  return false;
}

// Emit a sequence of instructions to compare LHS and RHS using Pred. Return
// the result in DstReg.
// FIXME: Maybe we should expand this earlier.
bool CapstoneInstructionSelector::selectFPCompare(MachineInstr &MI,
                                               MachineIRBuilder &MIB) const {
  auto &CmpMI = cast<GFCmp>(MI);
  CmpInst::Predicate Pred = CmpMI.getCond();

  Register DstReg = CmpMI.getReg(0);
  Register LHS = CmpMI.getLHSReg();
  Register RHS = CmpMI.getRHSReg();

  unsigned Size = MRI->getType(LHS).getSizeInBits();
  assert((Size == 16 || Size == 32 || Size == 64) && "Unexpected size");

  Register TmpReg = DstReg;

  bool NeedInvert = false;
  // First try swapping operands or inverting.
  if (legalizeFCmpPredicate(LHS, RHS, Pred, NeedInvert)) {
    if (NeedInvert)
      TmpReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
    auto Cmp = MIB.buildInstr(getFCmpOpcode(Pred, Size), {TmpReg}, {LHS, RHS});
    if (!Cmp.constrainAllUses(TII, TRI, RBI))
      return false;
  } else if (Pred == CmpInst::FCMP_ONE || Pred == CmpInst::FCMP_UEQ) {
    // fcmp one LHS, RHS => (OR (FLT LHS, RHS), (FLT RHS, LHS))
    NeedInvert = Pred == CmpInst::FCMP_UEQ;
    auto Cmp1 = MIB.buildInstr(getFCmpOpcode(CmpInst::FCMP_OLT, Size),
                               {&Capstone::GPRRegClass}, {LHS, RHS});
    if (!Cmp1.constrainAllUses(TII, TRI, RBI))
      return false;
    auto Cmp2 = MIB.buildInstr(getFCmpOpcode(CmpInst::FCMP_OLT, Size),
                               {&Capstone::GPRRegClass}, {RHS, LHS});
    if (!Cmp2.constrainAllUses(TII, TRI, RBI))
      return false;
    if (NeedInvert)
      TmpReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
    auto Or =
        MIB.buildInstr(Capstone::OR, {TmpReg}, {Cmp1.getReg(0), Cmp2.getReg(0)});
    if (!Or.constrainAllUses(TII, TRI, RBI))
      return false;
  } else if (Pred == CmpInst::FCMP_ORD || Pred == CmpInst::FCMP_UNO) {
    // fcmp ord LHS, RHS => (AND (FEQ LHS, LHS), (FEQ RHS, RHS))
    // FIXME: If LHS and RHS are the same we can use a single FEQ.
    NeedInvert = Pred == CmpInst::FCMP_UNO;
    auto Cmp1 = MIB.buildInstr(getFCmpOpcode(CmpInst::FCMP_OEQ, Size),
                               {&Capstone::GPRRegClass}, {LHS, LHS});
    if (!Cmp1.constrainAllUses(TII, TRI, RBI))
      return false;
    auto Cmp2 = MIB.buildInstr(getFCmpOpcode(CmpInst::FCMP_OEQ, Size),
                               {&Capstone::GPRRegClass}, {RHS, RHS});
    if (!Cmp2.constrainAllUses(TII, TRI, RBI))
      return false;
    if (NeedInvert)
      TmpReg = MRI->createVirtualRegister(&Capstone::GPRRegClass);
    auto And =
        MIB.buildInstr(Capstone::AND, {TmpReg}, {Cmp1.getReg(0), Cmp2.getReg(0)});
    if (!And.constrainAllUses(TII, TRI, RBI))
      return false;
  } else
    llvm_unreachable("Unhandled predicate");

  // Emit an XORI to invert the result if needed.
  if (NeedInvert) {
    auto Xor = MIB.buildInstr(Capstone::XORI, {DstReg}, {TmpReg}).addImm(1);
    if (!Xor.constrainAllUses(TII, TRI, RBI))
      return false;
  }

  MI.eraseFromParent();
  return true;
}

void CapstoneInstructionSelector::emitFence(AtomicOrdering FenceOrdering,
                                         SyncScope::ID FenceSSID,
                                         MachineIRBuilder &MIB) const {
  if (STI.hasStdExtZtso()) {
    // The only fence that needs an instruction is a sequentially-consistent
    // cross-thread fence.
    if (FenceOrdering == AtomicOrdering::SequentiallyConsistent &&
        FenceSSID == SyncScope::System) {
      // fence rw, rw
      MIB.buildInstr(Capstone::FENCE, {}, {})
          .addImm(CapstoneFenceField::R | CapstoneFenceField::W)
          .addImm(CapstoneFenceField::R | CapstoneFenceField::W);
      return;
    }

    // MEMBARRIER is a compiler barrier; it codegens to a no-op.
    MIB.buildInstr(TargetOpcode::MEMBARRIER, {}, {});
    return;
  }

  // singlethread fences only synchronize with signal handlers on the same
  // thread and thus only need to preserve instruction order, not actually
  // enforce memory ordering.
  if (FenceSSID == SyncScope::SingleThread) {
    MIB.buildInstr(TargetOpcode::MEMBARRIER, {}, {});
    return;
  }

  // Refer to Table A.6 in the version 2.3 draft of the Capstone Instruction Set
  // Manual: Volume I.
  unsigned Pred, Succ;
  switch (FenceOrdering) {
  default:
    llvm_unreachable("Unexpected ordering");
  case AtomicOrdering::AcquireRelease:
    // fence acq_rel -> fence.tso
    MIB.buildInstr(Capstone::FENCE_TSO, {}, {});
    return;
  case AtomicOrdering::Acquire:
    // fence acquire -> fence r, rw
    Pred = CapstoneFenceField::R;
    Succ = CapstoneFenceField::R | CapstoneFenceField::W;
    break;
  case AtomicOrdering::Release:
    // fence release -> fence rw, w
    Pred = CapstoneFenceField::R | CapstoneFenceField::W;
    Succ = CapstoneFenceField::W;
    break;
  case AtomicOrdering::SequentiallyConsistent:
    // fence seq_cst -> fence rw, rw
    Pred = CapstoneFenceField::R | CapstoneFenceField::W;
    Succ = CapstoneFenceField::R | CapstoneFenceField::W;
    break;
  }
  MIB.buildInstr(Capstone::FENCE, {}, {}).addImm(Pred).addImm(Succ);
}

namespace llvm {
InstructionSelector *
createCapstoneInstructionSelector(const CapstoneTargetMachine &TM,
                               const CapstoneSubtarget &Subtarget,
                               const CapstoneRegisterBankInfo &RBI) {
  return new CapstoneInstructionSelector(TM, Subtarget, RBI);
}
} // end namespace llvm
